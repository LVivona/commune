{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Transformer Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import commune as c\n",
    "c.enable_jupyter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the model.transformers Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__init__': {'input': {'config': 'NA'}, 'output': {}, 'type': 'self'},\n",
       " 'forward': {'input': {'input_ids': 'union[str, torch',\n",
       "   'attention_mask': 'torch.Tensor',\n",
       "   'return_keys': 'list[str]',\n",
       "   'topk': 'int',\n",
       "   'hidden_layer': 'int',\n",
       "   'max_sequence_length': 'int'},\n",
       "  'output': {},\n",
       "  'type': 'self'},\n",
       " 'encode': {'input': {'text': 'str', 'token_idx': 'int'},\n",
       "  'output': 'torch.Tensor',\n",
       "  'type': 'self'},\n",
       " 'embed': {'input': {'text': 'str', 'token_idx': 'int'},\n",
       "  'output': {},\n",
       "  'type': 'self'},\n",
       " 'set_model': {'input': {'config': 'NA'}, 'output': 'None', 'type': 'self'},\n",
       " 'set_tokenizer': {'input': {'tokenizer': 'str'},\n",
       "  'output': {},\n",
       "  'type': 'self'},\n",
       " 'encode_topk': {'input': {'forward_response_tensor': 'torch.Tensor',\n",
       "   'topk': 'int'},\n",
       "  'output': 'torch.Tensor',\n",
       "  'type': 'static'},\n",
       " 'decode_topk': {'input': {'forward_response_tensor': 'torch.Tensor',\n",
       "   'vocab_size': 'int',\n",
       "   'topk': 'NA'},\n",
       "  'output': 'torch.Tensor',\n",
       "  'type': 'static'},\n",
       " 'tokenizer_name': {'input': {}, 'output': {}, 'type': 'self'},\n",
       " 'tokenize': {'input': {'text': 'str',\n",
       "   'device': 'str',\n",
       "   'padding': 'NA',\n",
       "   'truncation': 'NA',\n",
       "   'max_length': 'NA',\n",
       "   'return_tensors': 'NA',\n",
       "   'add_special_tokens': 'NA'},\n",
       "  'output': 'torch.Tensor',\n",
       "  'type': 'self'},\n",
       " 'detokenize': {'input': {'input_ids': 'torch.Tensor'},\n",
       "  'output': 'torch.Tensor',\n",
       "  'type': 'self'},\n",
       " 'sample_check': {'input': {'sample': 'NA'}, 'output': {}, 'type': 'cls'},\n",
       " 'async_get_sample': {'input': {'dataset': 'NA',\n",
       "   'max_trials': 'NA',\n",
       "   'batch_size': 'NA',\n",
       "   'sequence_length': 'NA',\n",
       "   'num_batches': 'NA'},\n",
       "  'output': {},\n",
       "  'type': 'cls'},\n",
       " 'get_sample': {'input': {'timeout': 'NA', 'retries': 'NA'},\n",
       "  'output': {},\n",
       "  'type': 'cls'},\n",
       " 'resolve_model': {'input': {'model': 'NA'}, 'output': {}, 'type': 'cls'},\n",
       " 'test_encode': {'input': {'num_samples': 'int', 'text': 'NA'},\n",
       "  'output': {},\n",
       "  'type': 'cls'},\n",
       " 'serve': {'input': {'model': 'str', 'tag': 'NA', 'refresh': 'NA'},\n",
       "  'output': {},\n",
       "  'type': 'cls'},\n",
       " 'calculate_loss': {'input': {'logits': 'torch.Tensor',\n",
       "   'input_ids': 'torch.Tensor',\n",
       "   'return_value': 'NA'},\n",
       "  'output': 'torch.Tensor',\n",
       "  'type': 'cls'},\n",
       " 'generate': {'input': {'text': 'str',\n",
       "   'max_length': 'int',\n",
       "   'max_new_tokens': 'int',\n",
       "   'min_length': 'int',\n",
       "   'min_new_tokens': 'int',\n",
       "   'early_stopping': 'bool',\n",
       "   'max_time': 'float'},\n",
       "  'output': 'list[str]',\n",
       "  'type': 'self'},\n",
       " 'test_generate': {'input': {}, 'output': {}, 'type': 'cls'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = c.module('model.transformer')\n",
    "# see the schema if you want\n",
    "model.schema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Model shortcuts\n",
    "\n",
    "Managing module shortcuts help you simplify the naming of long ass hf model paths. We have done it for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'oa.pythia.12b': 'OpenAssistant/oasst-sft-1-pythia-12b',\n",
       " 'vicuna.33b': 'lmsys/vicuna-33b-v1.3',\n",
       " 'vicuna.13b': 'lmsys/vicuna-13b-v1.3',\n",
       " 'vicuna.7b': 'lmsys/vicuna-7b-v1.3',\n",
       " 'llama': 'openlm-research/open_llama_7b',\n",
       " 'llama2.70b': 'meta-llama/Llama-2-70b-chat-hf',\n",
       " 'llama2.7b': 'meta-llama/Llama-2-7b',\n",
       " 'llama13b': 'openlm-research/open_llama_13b',\n",
       " 'llama-trl': 'trl-lib/llama-7b-se-rl-peft',\n",
       " 'pygmalion-6b': 'Pygmalionreplit/replit-code-v1-3bAI/pygmalion-6b',\n",
       " 'stablellm7b': 'StabilityAI/stablelm-tuned-alpha-7b',\n",
       " 'mosaic': 'mosaicml/mpt-30b',\n",
       " 'falcon_40b_instruct': 'tiiuae/falcon-40b-instruct',\n",
       " 'falcon_uncensored': 'ehartford/WizardLM-Uncensored-Falcon-40b',\n",
       " 'falcon': 'tiiuae/falcon-40b-instruct',\n",
       " 'open_llama_13b': 'openlm-research/open_llama_13b',\n",
       " 'lazarus30b': 'CalderaAI/30B-Lazarus',\n",
       " 'starcoder': 'bigcode/starcoder',\n",
       " 'wizardcoder': 'WizardLM/WizardCoder-15B-V1.0',\n",
       " 'replit_code': 'replit/replit-code-v1-3b'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.shortcuts.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shortcut = 'gpt2'\n",
    "path = 'OpenAI/gpt-2'\n",
    "c.setc(f'shortcuts.{shortcut}', path) \n",
    "assert c.getc(f'shortcuts.{shortcut}') == path\n",
    "c.rmc(f'shortcuts.{shortcut}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt2'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
