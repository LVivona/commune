
vocab_size: 50432
hidden_size: 6144
num_hidden_layers: 10
# num_hidden_layers: 2
quantize: True
init_weights: True
init_empty_weights: False
num_attention_heads: 64
intermediate_size_factor: 4
hidden_act: gelu
rotary_pct: 0.25
rotary_emb_base: 10000
max_position_embeddings: 2048
initializer_range: 0.02
layer_norm_eps: 0.00005
use_cache: True
bos_token_id: 0
eos_token_id: 2
tie_word_embeddings: False
use_return_dict: True
use_parallel_residual: True
output_attentions: False
output_hidden_states: False
pruned_heads : null
no_split_modules : ["GPTNeoXLayer"]
max_gpu_ratio: 0.4
load_weights: gpt20b