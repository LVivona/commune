
model: gpt125m
patience: 3
tokenizer: null
epoch_length: 10
save: true # save on epoch 
device: null
epoch_length: 100
max_gpu_ratio: 0.8
finetune: 1
optimizer:
  lr: 1.0e-05
  module: torch.optim.Adam
load_in_8bit: False
stats: {}
max_gpu_ratio: 0.8
alpha: 0.9
model_inflation_ratio: 1.0
output_length: 4
max_sequence_length: 256
tag: null
load: true
device_map: null
max_memory: null
reserve_gpus: False
verbose: true
test: true


