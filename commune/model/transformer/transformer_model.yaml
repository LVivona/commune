
model: gpt125m
tokenizer: null
save: true # save on epoch 
device: null
epoch_length: 100
max_gpu_ratio: 0.8
finetune: 1
optimizer:
  lr: 1.0e-05
  module: torch.optim.Adam
load_in_8bit: False
stats: {}
max_gpu_ratio: 0.8
evaluation_steps: 20
loss_history_size: 100
model_inflation_ratio: 1.5
output_length: 4
max_sequence_length: 256
tag: null
load: true
device_map: null
max_memory: null
reserve_gpus: False
verbose: true
test: false
train: False
min_steps_since_save: 100 # save at most every 50 steps
max_epochs_since_saved: 2
min_epochs_since_saved: 1
default_metric: 12
hidden_size: 768
vocab_size: 50258
loss_sigdigs: 3




