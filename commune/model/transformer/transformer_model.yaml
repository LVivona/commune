
model: gpt125m
tokenizer: null
save: true # save on epoch 
device: null
stats: null
epoch_length: 100
patience_epochs: 2
best_loss_delta: -0.1
default_metric: 12
hidden_size: 768
vocab_size: 50258
loss_sigdigs: 3
max_gpu_ratio: 0.8
trust_remote_code: True
finetune: 2
optimizer:
  lr: 1.0e-05
  module: torch.optim.Adam
load_in_8bit: False
stats: {}
model_inflation_ratio: 1.5
output_length: 4
max_sequence_length: 256
clip_grad_norm: 1.0
tag: null
load: false
device_map: null
max_memory: null
reserve_gpus: False
verbose: true
test: false
train: True
max_epochs_since_saved: 2
default_metric: 12
hidden_size: 768
vocab_size: 50258
loss_sigdigs: 3




