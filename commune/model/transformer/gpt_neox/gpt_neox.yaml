
module: 
vocab_size: 50432
hidden_size: 6144
num_hidden_layers: 44
init_weights: True
num_attention_heads: 64
intermediate_size: 4
hidden_act: gelu
rotary_pct: 0.25
rotary_emb_base: 10000
max_position_embeddings: 2048
initializer_range: 0.02
layer_norm_eps: 1e-5
use_cache: True
bos_token_id: 0
eos_token_id: 2
tie_word_embeddings: False
use_parallel_residual: True
pruned_heads : null
no_split_modules : ["GPTNeoXLayer"]