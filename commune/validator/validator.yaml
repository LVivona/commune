models: model
tokenizer: gpt2
train: False
batch_size: 32
sequence_length: 256
network: global
dataset: dataset.bittensor
metric:  null
stats: {}
threshold: 4.0
max_stats_history: 100
alpha: 0.1
selection_ratio: 1.0
topk: 512 
load: False
new_loop_per_forward: True
hidden_size : 512
